{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and manage Segment Anything\n",
    "\n",
    "[Segment Anything](https://segment-anything.com/) (SA-1B) is a image segmentation dataset containing 1B masks and 11M images. The source dataset has the following file layout:\n",
    "\n",
    "```\n",
    "sa_000000/sa_1.json\n",
    "         /sa_2.json\n",
    "         ...\n",
    "sa_000001/...\n",
    "```\n",
    "\n",
    "where each JSON file contains an image information and its labels, the equivalent PyArrow schema for the JSON file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "segmentation_schema = pa.struct([\n",
    "  (\"size\", pa.list_(pa.int64())),\n",
    "  (\"counts\", pa.string())\n",
    "])\n",
    "\n",
    "annotation_schema = pa.struct([\n",
    "  (\"area\", pa.int64()),\n",
    "  (\"bbox\", pa.list_(pa.float64())),\n",
    "  (\"crop_box\", pa.list_(pa.float64())),\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"point_coords\", pa.list_(pa.list_(pa.float64()))),\n",
    "  (\"predicted_iou\", pa.float64()),\n",
    "  (\"segmentation\", segmentation_schema),\n",
    "  (\"stability_score\", pa.float64())\n",
    "])\n",
    "\n",
    "image_schema = pa.struct([\n",
    "  (\"image_id\", pa.int64()),\n",
    "  (\"file_name\", pa.string()),\n",
    "  (\"width\", pa.int64()),\n",
    "  (\"height\", pa.int64())\n",
    "])\n",
    "\n",
    "ds_schema = pa.schema([\n",
    "  (\"image_id\", pa.int64()),  # Add an image_id as the primary key.\n",
    "  (\"shard\", pa.string()),  # Record shard (\"sa_000000\") for inferring full image path.\n",
    "  (\"image\", image_schema),\n",
    "  (\"annotations\", pa.list_(annotation_schema))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts the JSON objects to an Arrow table. Each row represents an image with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pyarrow import json\n",
    "\n",
    "sa_dir = \"/space/segmeng_anything\"  # Change to your data folder.\n",
    "# Knobs for making things faster.\n",
    "images_per_shard = 100\n",
    "total_shards = 10\n",
    "\n",
    "\n",
    "def make_iter(shards: List[str]) -> Iterator[pa.Table]:\n",
    "\n",
    "  def json_files_to_arrow(shard: str) -> pa.Table:\n",
    "    batch: List[pa.Table] = []\n",
    "    pattern = os.path.join(sa_dir, shard, \"*.json\")\n",
    "    print(f\"processing pattern: {pattern}\")\n",
    "    for f in glob.glob(pattern):\n",
    "      batch.append(json.read_json(f,\n",
    "        parse_options=json.ParseOptions(explicit_schema=ds_schema)))\n",
    "      if len(batch) >= images_per_shard:\n",
    "        break\n",
    "\n",
    "    table = pa.concat_tables(batch)\n",
    "    image_id_column = table.column(\"image\").combine_chunks().field(\"image_id\")\n",
    "    shard_column = pa.StringArray.from_pandas([shard] * table.num_rows)\n",
    "\n",
    "    # Add image_id and shard columns to the original data.\n",
    "    table = table.drop(\"shard\").append_column(\n",
    "      pa.field(\"shard\", pa.string()), shard_column)\n",
    "    table = table.drop(\"image_id\").append_column(\n",
    "      pa.field(\"image_id\", pa.int64()), image_id_column)\n",
    "    return table\n",
    "\n",
    "  for shard in shards:\n",
    "    yield json_files_to_arrow(shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import Dataset\n",
    "\n",
    "ds_location = \"/space/datasets/sa\"\n",
    "# There is no record field, because this dataset does not store image bytes.\n",
    "ds = Dataset.create(ds_location, ds_schema,\n",
    "  primary_keys=[\"image_id\"], record_fields=[])\n",
    "\n",
    "ds = Dataset.load(ds_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data into it distributedly using Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ray().append_from(\n",
    "  [lambda: make_iter([f\"sa_{i:06}\"]) for i in range(total_shards)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading image information and annotations into Space, next step we read and preprocess images, then persist the result in Space as Materialized Views. When the source dataset is modified, we can refresh the image dataset to incrementally synchronizes changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import cv2\n",
    "\n",
    "def read_and_preprocess_image(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  ims = []\n",
    "  for image_id, shard, image in zip(data[\"image_id\"], data[\"shard\"], data[\"image\"]):\n",
    "    full_path = os.path.join(sa_dir, shard, image[\"file_name\"])\n",
    "    im = cv2.imread(full_path)\n",
    "    im = cv2.resize(im, dsize=(100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "    ims.append(im.tobytes())\n",
    "\n",
    "  # Drop `image` column that won't writer into the output view, add a new\n",
    "  # `image_bytes` column.\n",
    "  del data[\"image\"]\n",
    "  data[\"image_bytes\"] = ims\n",
    "  return data\n",
    "\n",
    "\n",
    "# Create a view by transforming the source dataset.\n",
    "view = ds.map_batches(\n",
    "  fn=read_and_preprocess_image,\n",
    "  input_fields=[\"image_id\", \"shard\", \"image\"], # Don't need to read annotations.\n",
    "  output_schema=pa.schema([\n",
    "    (\"image_id\", pa.int64()),\n",
    "    (\"shard\", pa.string()),\n",
    "    (\"image_bytes\", pa.binary())  # Add a new field for image bytes.\n",
    "  ]),\n",
    "  output_record_fields=[\"image_bytes\"] # Store this field in ArrayRecord.\n",
    ")\n",
    "# Create an empty materialized view.\n",
    "mv = view.materialize(\"/space/datasets/sa_mv\")\n",
    "\n",
    "# Synchronize the MV to the source dataset.\n",
    "mv.ray().refresh()\n",
    "\n",
    "# Verifly the content in mv\n",
    "mv.ray().read_all().num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some modifications in the source dataset, and synchronize changes to MV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all image IDs, and pick a few to delete.\n",
    "ds.local().read_all().select([\"image_id\"])\n",
    "\n",
    "# Delete two images.\n",
    "ds.local().delete((pc.field(\"image_id\") == 4811) | (pc.field(\"image_id\") == 6973))\n",
    "\n",
    "mv.ray().refresh()\n",
    "# The images are deleted in MV as well.\n",
    "mv.local().read_all().select([\"image_id\"]).num_rows"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
