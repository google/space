{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load custom data and build transform pipeline: Segment Anything as example\n",
    "\n",
    "[Segment Anything](https://segment-anything.com/) (SA-1B) is an image segmentation dataset containing 1B masks and 11M images. The source dataset has the following file layout:\n",
    "\n",
    "```\n",
    "sa_000000/sa_1.json\n",
    "         /sa_1.jpg\n",
    "         /sa_2.json\n",
    "         /sa_2.jpg\n",
    "         ...\n",
    "sa_000001/...\n",
    "```\n",
    "\n",
    "where each JSON file contains an image information and its annotations. This example builds a mini pipeline that loads, processes, manages, and feeds data to ML frameworks. We add more complexities to show the features of Space:\n",
    "\n",
    "- JSON data are loaded to a `raw dataset` stored in Parquet. Images are not loaded. This dataset is good for running data mutations and OLAP queries on annotations.\n",
    "- The raw dataset is transformed to a `preprocessed images materialized view` stored in [ArrayRecord](https://github.com/google/array_record). When the raw dataset is modified, refreshing the materialized view (MV) synchronizes changes incrementally. Because annotations are excluded here, the images can be shared by different training tasks, by joining with different annotation tables.\n",
    "- Similarly, the raw dataset is also transformed to an `annotations materialized view`.\n",
    "- Both MVs are keyed by `image_id`, they are joined (low cost JOIN of row addresses instead of real data) to re-construct the `<image, annotation>` pairs. A PyTorch style random access data source is built from the JOIN result, for feeding data to ML frameworks.\n",
    "\n",
    "The idea is shown in the following pic:\n",
    "\n",
    "<div>\n",
    "<img src=\"pics/space_segment_anything_example.png\" width=\"600\" style=\"float: center;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Load Raw Data into Space Datasets\n",
    "\n",
    "The first steps load JOSN files into a Space dataset. The equivalent PyArrow schema for the Space dataset/JSON file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "segmentation_schema = pa.struct([\n",
    "  (\"size\", pa.list_(pa.int64())),\n",
    "  (\"counts\", pa.string())\n",
    "])\n",
    "\n",
    "annotation_schema = pa.struct([\n",
    "  (\"area\", pa.int64()),\n",
    "  (\"bbox\", pa.list_(pa.float64())),\n",
    "  (\"crop_box\", pa.list_(pa.float64())),\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"point_coords\", pa.list_(pa.list_(pa.float64()))),\n",
    "  (\"predicted_iou\", pa.float64()),\n",
    "  (\"segmentation\", segmentation_schema),\n",
    "  (\"stability_score\", pa.float64())\n",
    "])\n",
    "\n",
    "image_schema = pa.struct([\n",
    "  (\"image_id\", pa.int64()),\n",
    "  (\"file_name\", pa.string()),\n",
    "  (\"width\", pa.int64()),\n",
    "  (\"height\", pa.int64())\n",
    "])\n",
    "\n",
    "# This is the top level dataset schema.\n",
    "ds_schema = pa.schema([\n",
    "  (\"image_id\", pa.int64()),  # Add an image_id as the primary key.\n",
    "  (\"shard\", pa.string()),  # Add shard (\"sa_000000\") for inferring full image path later.\n",
    "  (\"image\", image_schema),\n",
    "  (\"annotations\", pa.list_(annotation_schema))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method reads JSON files into Arrow tables, one row per file. Each row represents an image with annotations. It simply uses `pyarrow.json.read_json` to load JSON to Arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pyarrow import json\n",
    "\n",
    "sa_dir = \"/segment_anything\"  # Change to your data folder.\n",
    "# Knobs for making things faster.\n",
    "images_per_shard = 100\n",
    "total_shards = 10\n",
    "\n",
    "\n",
    "def make_iter(shards: List[str]) -> Iterator[pa.Table]:\n",
    "\n",
    "  def json_files_to_arrow(shard: str) -> pa.Table:\n",
    "    batch: List[pa.Table] = []\n",
    "\n",
    "    pattern = os.path.join(sa_dir, shard, \"*.json\")\n",
    "    print(f\"processing pattern: {pattern}\")\n",
    "    for f in glob.glob(pattern):\n",
    "      batch.append(json.read_json(f,\n",
    "        parse_options=json.ParseOptions(explicit_schema=ds_schema)))\n",
    "      if len(batch) >= images_per_shard:\n",
    "        break\n",
    "\n",
    "    table = pa.concat_tables(batch)\n",
    "    image_id_column = table.column(\"image\").combine_chunks().field(\"image_id\")\n",
    "    shard_column = pa.StringArray.from_pandas([shard] * table.num_rows)\n",
    "\n",
    "    # Add image_id and shard columns to the original data.\n",
    "    table = table.drop(\"shard\").append_column(\n",
    "      pa.field(\"shard\", pa.string()), shard_column)\n",
    "    table = table.drop(\"image_id\").append_column(\n",
    "      pa.field(\"image_id\", pa.int64()), image_id_column)\n",
    "    return table\n",
    "\n",
    "  for shard in shards:\n",
    "    yield json_files_to_arrow(shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import Dataset\n",
    "\n",
    "raw_ds_location = \"/space/datasets/raw_data\"\n",
    "\n",
    "# `record_fields` specifies which fields to store in ArrayRecord.\n",
    "# The raw dataset is pure Parquet, so leave it empty.\n",
    "raw_ds = Dataset.create(raw_ds_location, ds_schema,\n",
    "  primary_keys=[\"image_id\"], record_fields=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the dataset can be loaded from location later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = Dataset.load(raw_ds_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start loading JSON data into Space distributedly using Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append_from accepts a list of no-args methods that returns a generator.\n",
    "# A method is used because generator cannot be pickled for distributed execuion.\n",
    "raw_ds.ray().append_from(\n",
    "  [lambda idx=i: make_iter([f\"sa_{idx:06}\"]) for i in range(total_shards)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processed Image MVs\n",
    "\n",
    "After loading image information and annotations into Space, next step we read and pre-process images, then persist the result in a Space Materialized Views (MV).\n",
    "\n",
    "First define a function to load and process images for a batch of inputs. The Ray runner uses [Ray map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) internally, so the input format is `Dict[str, np.ndarray]`, where key is field names in the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import cv2\n",
    "\n",
    "def read_and_preprocess_image(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  ims = []\n",
    "  for image_id, shard, image in zip(data[\"image_id\"], data[\"shard\"], data[\"image\"]):\n",
    "    full_path = os.path.join(sa_dir, shard, image[\"file_name\"])\n",
    "    im = cv2.imread(full_path)\n",
    "    im = cv2.resize(im, dsize=(100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "    ims.append(im.tobytes())\n",
    "\n",
    "  # Drop `image` column that won't writer into the output view, add a new\n",
    "  # `image_bytes` column.\n",
    "  del data[\"image\"]\n",
    "  data[\"image_bytes\"] = ims\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view `image_view` by transforming the raw dataset using the function above. By providing an `input_fields`, we only read these fields from the dataset to save IO (annotations are not read). The output schema must be compatible with the data returned by `read_and_preprocess_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view by transforming the source dataset.\n",
    "image_view = raw_ds.map_batches(\n",
    "  fn=read_and_preprocess_image,\n",
    "  input_fields=[\"image_id\", \"shard\", \"image\"], # Don't need to read annotations.\n",
    "  output_schema=pa.schema([\n",
    "    (\"image_id\", pa.int64()),\n",
    "    (\"shard\", pa.string()),\n",
    "    (\"image_bytes\", pa.binary())  # Add a new field for image bytes.\n",
    "  ]),\n",
    "  output_record_fields=[\"image_bytes\"]  # Store this field in ArrayRecord.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries on `image_view` reads all data from `raw_ds` and running the transform again. To avoid that, we can **materialize** the view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty MV.\n",
    "image_mv_location = \"/space/mvs/images\"\n",
    "image_mv = image_view.materialize(image_mv_location)\n",
    "\n",
    "# Synchronize the MV to the source dataset. It populates the MV storage.\n",
    "image_mv.ray().refresh()\n",
    "\n",
    "# Verifly the images in MV.\n",
    "image_mv.ray().read_all().num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as datasets, MVs can be loaded from file location anytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import MaterializedView\n",
    "\n",
    "image_mv = MaterializedView.load(image_mv_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some modifications in the source dataset, and synchronize changes to MV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all image IDs, and pick a few to delete.\n",
    "raw_ds.local().read_all().select([\"image_id\"])\n",
    "\n",
    "# Delete two images.\n",
    "raw_ds.local().delete((pc.field(\"image_id\") == 4811) | (pc.field(\"image_id\") == 6973))\n",
    "\n",
    "image_mv.ray().refresh()\n",
    "# The images are deleted in MV.\n",
    "image_mv.local().read_all().select([\"image_id\"]).num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations MVs\n",
    "\n",
    "This step creates a MV for annotations. Because there can be hundreds of objects with segmentation data in annotations, its size is much larger than other datasets. This example stores serialized annotations (`annotations_bytes`) in ArrayRecord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def serialize_annotations(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  # pickle is just an example of serialization.\n",
    "  data[\"annotations_bytes\"] = [pickle.dumps(d) for d in data[\"annotations\"]]\n",
    "  return data\n",
    "\n",
    "\n",
    "# Create a view by transforming the source dataset.\n",
    "annotations_view = ds.map_batches(\n",
    "  fn=serialize_annotations,\n",
    "  input_fields=[\"image_id\", \"annotations\"],\n",
    "  output_schema=pa.schema([\n",
    "    (\"image_id\", pa.int64()),\n",
    "    (\"annotations_bytes\", pa.binary())\n",
    "  ]),\n",
    "  output_record_fields=[\"annotations_bytes\"]\n",
    ")\n",
    "\n",
    "# Create an empty MV.\n",
    "annotations_mv_location = \"/space/mvs/annotations\"\n",
    "annotations_mv = annotations_view.materialize(annotations_mv_location)\n",
    "\n",
    "annotations_mv.ray().refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Images and Annotations data\n",
    "\n",
    "Images and annotations are now stored separately. At training time, we have to pair they together and feed them to the training framework. Both images and annotations MVs have primary key `image_id`, so let's join them.\n",
    "\n",
    "JOIN is relatively cheap in Space, because it can:\n",
    "\n",
    "- Read row addresses of records in ArrayRecord files instead of data.\n",
    "- Joining addresses.\n",
    "- Build a random access data source on top of the JOIN result, which auto reads data from the addresses.\n",
    "\n",
    "When the data to join does not fit into memory, we can use a `partitioned JOIN` to run it on multiple workers. We plan to support it natively in Space, but this tutorial has to do it more manually:\n",
    "\n",
    "- For the primary key `image_id`, find out its min and max across all files, by reading the manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema to know field ID to name mapping.\n",
    "# It shows the field ID of `image_id` is 0.\n",
    "print(annotations_view.view.schema)\n",
    "\n",
    "# Space stores column statistics in a Parquet table read by `index_manifest`.\n",
    "# `_STATS_f0` in the manifest table stores stats for field `image_id`, where `0` is field ID.\n",
    "stats = pa.concat_tables(\n",
    "  mv_annotations.storage.index_manifest()).column(\"_STATS_f0\").combine_chunks()\n",
    "\n",
    "# This is the full range of `image_id`.\n",
    "min_ = min(stats.field(\"_MIN\").to_pylist())\n",
    "max_ = max(stats.field(\"_MAX\").to_pylist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we have N workers, split the min max range into N ranges. Each worker is responsible for one range:\n",
    "  - The assigned range is converted to a filter when reading data.\n",
    "  - The filter is pushed down to file readers, uninterested files are directly skipped, see details in [this paper](https://vldb.org/pvldb/vol14/p3083-edara.pdf).\n",
    "- Perform JOIN locally on each worker, and aggregate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "def join_image_and_annotations(\n",
    "    location_image: str, location_annotations: str,\n",
    "    min_: int, max_: int, last: bool):\n",
    "  mv_image = MaterializedView.load(location_image)\n",
    "  mv_annotations = MaterializedView.load(location_annotations)\n",
    "\n",
    "  if not last:\n",
    "    filter_ = (pc.field(\"image_id\") >= min_) & (pc.field(\"image_id\") < max_)\n",
    "  else:\n",
    "    filter_ = (pc.field(\"image_id\") >= min_) & (pc.field(\"image_id\") <= max_)\n",
    "\n",
    "  # `reference_read=True` reads tuple (file_path, row_id) of a record, instead of data content.\n",
    "  image_refs = mv_image.local().read_all(filter_, reference_read=True).flatten()\n",
    "  annotations_refs = mv_annotations.local().read_all(filter_, reference_read=True).flatten()\n",
    "  return image_refs.join(annotations_refs, keys=\"image_id\")\n",
    "\n",
    "\n",
    "# This example skips the details of splitting the full range, here use [0, 10000) as an example\n",
    "# of a JOIN partition.\n",
    "joined_addresses = join_image_and_annotations(\n",
    "  image_mv_location,\n",
    "  annotations_mv_location,\n",
    "  # A range assigned to a partitioned JOIN.\n",
    "  min_=0, max_=10000, last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JOIN result table contains record addresses of `image_bytes` and `annotations_bytes`. Space provides a PyTorch style random access data source for reading the addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import RandomAccessDataSource\n",
    "\n",
    "datasource = RandomAccessDataSource(\n",
    "  # <field-name>: <storage-location>, for reading data from ArrayRecord files.\n",
    "  {\n",
    "    \"image_bytes\": image_mv_location,\n",
    "    \"annotations_bytes\": annotations_mv_location,\n",
    "  },\n",
    "  joined_addresses,\n",
    "  # Don't auto deserialize data, because we store them as plain bytes.\n",
    "  deserialize=False)\n",
    "\n",
    "# Total data size.\n",
    "len(datasource)\n",
    "\n",
    "# Random access records.\n",
    "datasource[89].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example stores annotations and images as bytes. Space has build-in support for [TFDS FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) to make serializing/deserializing data more managed, see the [TFDS example](tfds_coco_tutorial.ipynb). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
