{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and manage Segment Anything\n",
    "\n",
    "### Load Raw Data into Space Datasets\n",
    "\n",
    "[Segment Anything](https://segment-anything.com/) (SA-1B) is a image segmentation dataset containing 1B masks and 11M images. The source dataset has the following file layout:\n",
    "\n",
    "```\n",
    "sa_000000/sa_1.json\n",
    "         /sa_1.jpg\n",
    "         /sa_2.json\n",
    "         /sa_2.jpg\n",
    "         ...\n",
    "sa_000001/...\n",
    "```\n",
    "\n",
    "where each JSON file contains an image information and its labels, the equivalent PyArrow schema for the JSON file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "segmentation_schema = pa.struct([\n",
    "  (\"size\", pa.list_(pa.int64())),\n",
    "  (\"counts\", pa.string())\n",
    "])\n",
    "\n",
    "annotation_schema = pa.struct([\n",
    "  (\"area\", pa.int64()),\n",
    "  (\"bbox\", pa.list_(pa.float64())),\n",
    "  (\"crop_box\", pa.list_(pa.float64())),\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"point_coords\", pa.list_(pa.list_(pa.float64()))),\n",
    "  (\"predicted_iou\", pa.float64()),\n",
    "  (\"segmentation\", segmentation_schema),\n",
    "  (\"stability_score\", pa.float64())\n",
    "])\n",
    "\n",
    "image_schema = pa.struct([\n",
    "  (\"image_id\", pa.int64()),\n",
    "  (\"file_name\", pa.string()),\n",
    "  (\"width\", pa.int64()),\n",
    "  (\"height\", pa.int64())\n",
    "])\n",
    "\n",
    "ds_schema = pa.schema([\n",
    "  (\"image_id\", pa.int64()),  # Add an image_id as the primary key.\n",
    "  (\"shard\", pa.string()),  # Add shard (\"sa_000000\") for inferring full image path later.\n",
    "  (\"image\", image_schema),\n",
    "  (\"annotations\", pa.list_(annotation_schema))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method reads JSON files into Arrow tables, one row per file. Each row represents an image with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pyarrow import json\n",
    "\n",
    "sa_dir = \"/segment_anything\"  # Change to your data folder.\n",
    "# Knobs for making things faster.\n",
    "images_per_shard = 100\n",
    "total_shards = 10\n",
    "\n",
    "\n",
    "def make_iter(shards: List[str]) -> Iterator[pa.Table]:\n",
    "\n",
    "  def json_files_to_arrow(shard: str) -> pa.Table:\n",
    "    batch: List[pa.Table] = []\n",
    "\n",
    "    pattern = os.path.join(sa_dir, shard, \"*.json\")\n",
    "    print(f\"processing pattern: {pattern}\")\n",
    "    for f in glob.glob(pattern):\n",
    "      batch.append(json.read_json(f,\n",
    "        parse_options=json.ParseOptions(explicit_schema=ds_schema)))\n",
    "      if len(batch) >= images_per_shard:\n",
    "        break\n",
    "\n",
    "    table = pa.concat_tables(batch)\n",
    "    image_id_column = table.column(\"image\").combine_chunks().field(\"image_id\")\n",
    "    shard_column = pa.StringArray.from_pandas([shard] * table.num_rows)\n",
    "\n",
    "    # Add image_id and shard columns to the original data.\n",
    "    table = table.drop(\"shard\").append_column(\n",
    "      pa.field(\"shard\", pa.string()), shard_column)\n",
    "    table = table.drop(\"image_id\").append_column(\n",
    "      pa.field(\"image_id\", pa.int64()), image_id_column)\n",
    "    return table\n",
    "\n",
    "  for shard in shards:\n",
    "    yield json_files_to_arrow(shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import Dataset\n",
    "\n",
    "raw_ds_location = \"/space/datasets/raw_data\"\n",
    "# There is no record field, because this dataset does not store image bytes.\n",
    "raw_ds = Dataset.create(raw_ds_location, ds_schema,\n",
    "  primary_keys=[\"image_id\"], record_fields=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the dataset can be loaded from location later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = Dataset.load(raw_ds_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data into Space distributedly using Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append_from accepts a list of no-args methods that returns a generator.\n",
    "# A method is used because generator cannot be pickled for distributed execuion.\n",
    "raw_ds.ray().append_from(\n",
    "  [lambda idx=i: make_iter([f\"sa_{idx:06}\"]) for i in range(total_shards)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipelines of Ray Transforms and Materialized Views (MVs)\n",
    "\n",
    "Our goal in this example is to build two Space Materialized Views (MVs), for pre-processed images and annotations separately. When the source raw dataset is modified, the MVs can be refreshed to incrementally synchronize changes. Only the changes are processed.\n",
    "\n",
    "By separating images and annotations, the pre-processed image MV can be a central repository to be shared by multiple training tasks. It can be joined with other datasets or MVs of annotations to construct the final training input. Low cost JOINs are achieved by joining the reference (e.g., addresses in row format files) instead of data content. The JOINs can be effectively distributed using column statistics and data skipping technologies.\n",
    "\n",
    "#### Pre-processed Image MVs\n",
    "\n",
    "After loading image information and annotations into Space, next step we read and pre-process images, then persist the result in a Space Materialized Views (MV). When the source dataset is modified, we can refresh the MV to incrementally synchronizes changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import cv2\n",
    "\n",
    "def read_and_preprocess_image(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  ims = []\n",
    "  for image_id, shard, image in zip(data[\"image_id\"], data[\"shard\"], data[\"image\"]):\n",
    "    full_path = os.path.join(sa_dir, shard, image[\"file_name\"])\n",
    "    im = cv2.imread(full_path)\n",
    "    im = cv2.resize(im, dsize=(100, 100), interpolation=cv2.INTER_CUBIC)\n",
    "    ims.append(im.tobytes())\n",
    "\n",
    "  # Drop `image` column that won't writer into the output view, add a new\n",
    "  # `image_bytes` column.\n",
    "  del data[\"image\"]\n",
    "  data[\"image_bytes\"] = ims\n",
    "  return data\n",
    "\n",
    "\n",
    "# Create a view by transforming the source dataset.\n",
    "image_view = ds.map_batches(\n",
    "  fn=read_and_preprocess_image,\n",
    "  input_fields=[\"image_id\", \"shard\", \"image\"], # Don't need to read annotations.\n",
    "  output_schema=pa.schema([\n",
    "    (\"image_id\", pa.int64()),\n",
    "    (\"shard\", pa.string()),\n",
    "    (\"image_bytes\", pa.binary())  # Add a new field for image bytes.\n",
    "  ]),\n",
    "  output_record_fields=[\"image_bytes\"] # Store this field in ArrayRecord.\n",
    ")\n",
    "\n",
    "# Create an empty MV.\n",
    "image_mv_location = \"/space/mvs/image\"\n",
    "image_mv = image_view.materialize(image_mv_location)\n",
    "\n",
    "# Synchronize the MV to the source dataset. It populates the MV storage.\n",
    "image_mv.ray().refresh()\n",
    "\n",
    "# Verifly the images in MV.\n",
    "image_mv.ray().read_all().num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MV can be loaded from file location anytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import MaterializedView\n",
    "\n",
    "image_mv = MaterializedView.load(image_mv_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some modifications in the source dataset, and synchronize changes to MV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all image IDs, and pick a few to delete.\n",
    "raw_ds.local().read_all().select([\"image_id\"])\n",
    "\n",
    "# Delete two images.\n",
    "raw_ds.local().delete((pc.field(\"image_id\") == 4811) | (pc.field(\"image_id\") == 6973))\n",
    "\n",
    "image_mv.ray().refresh()\n",
    "# The images are deleted in MV.\n",
    "image_mv.local().read_all().select([\"image_id\"]).num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotations MVs\n",
    "\n",
    "In the next step, we create a MV for annotations. Annotations are stored as binary in ArrayRecord files, instead of columnar format in Parquet. There could be hundreds of object annotations per image, and annotation contains segmentation data that is much larger than class labels. Storing in ArrayRecord row format allows us to read annotations by reference instead of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def serialize_annotations(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  # pickle is just an example of serialization.\n",
    "  data[\"annotations_bytes\"] = [pickle.dumps(d) for d in data[\"annotations\"]]\n",
    "  return data\n",
    "\n",
    "\n",
    "# Create a view by transforming the source dataset.\n",
    "annotations_view = ds.map_batches(\n",
    "  fn=serialize_annotations,\n",
    "  input_fields=[\"image_id\", \"annotations\"],\n",
    "  output_schema=pa.schema([\n",
    "    (\"image_id\", pa.int64()),\n",
    "    (\"annotations_bytes\", pa.binary())\n",
    "  ]),\n",
    "  output_record_fields=[\"annotations_bytes\"]\n",
    ")\n",
    "\n",
    "# Create an empty MV.\n",
    "annotations_mv_location = \"/space/mvs/annotations\"\n",
    "annotations_mv = annotations_view.materialize(annotations_mv_location)\n",
    "\n",
    "annotations_mv.ray().refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Images and Annotations MVs\n",
    "\n",
    "Preprocessed images and its annotations are feed together to the training framework, after matching rows by `image_id` using a JOIN. JOIN is relatively cheap in Space, because it can:\n",
    "\n",
    "- Read references (addresses) of records instead of data.\n",
    "- Joining references and persist the result.\n",
    "- Scan the result, and read data from the references (de-reference).\n",
    "\n",
    "In addition, with the OLAP style column statistics, the JOIN can be easily parallelized on multiple workers:\n",
    "\n",
    "- For the primary key `image_id`, read its min and max across all files from manifests.\n",
    "- If we have N workers, split the min max range into N ranges. Each worker is responsible for one range.\n",
    "- On each worker, a filter is built based on the assigned range. The filter is pushed down to the data source. Because of the column statistics Space stored in manifest files, uninterested files are skipped (data skipping) to save IO cost.\n",
    "  - The column statistics can be read as follows. `_STATS_f0` is the stats for field ID `0`. Field ID to name mapping can be read from storage schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema to know field ID to name mapping.\n",
    "print(annotations_view.view.schema)\n",
    "\n",
    "# `_STATS_f0` stores stats for `image_id`.\n",
    "min_max = pa.concat_tables(\n",
    "  mv_annotations.storage.index_manifest()).column(\"_STATS_f0\").combine_chunks()\n",
    "\n",
    "# This is the full range to split.\n",
    "min_ = min(min_max.field(\"_MIN\").to_pylist())\n",
    "max_ = max(min_max.field(\"_MAX\").to_pylist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform JOIN locally on each worker, and aggregate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "def join_image_and_annotations(\n",
    "    location_image: str, location_annotations: str,\n",
    "    min_: int, max_: int, last: bool):\n",
    "  mv_image = MaterializedView.load(location_image)\n",
    "  mv_annotations = MaterializedView.load(location_annotations)\n",
    "\n",
    "  if not last:\n",
    "    filter_ = (pc.field(\"image_id\") >= min_) & (pc.field(\"image_id\") < max_)\n",
    "  else:\n",
    "    filter_ = (pc.field(\"image_id\") >= min_) & (pc.field(\"image_id\") <= max_)\n",
    "\n",
    "  # `reference_read=True` reads tuple (file_path, row_id) of a record, instead of data content.\n",
    "  image_refs = mv_image.local().read_all(filter_, reference_read=True).flatten()\n",
    "  annotations_refs = mv_annotations.local().read_all(filter_, reference_read=True).flatten()\n",
    "  return image_refs.join(annotations_refs, keys=\"image_id\")\n",
    "\n",
    "\n",
    "# This example skips the details of splitting the full range, here use [0, 10000) as an example\n",
    "# of a JOIN partition.\n",
    "joined_references = join_image_and_annotations(\n",
    "  image_mv_location,\n",
    "  annotations_mv_location,\n",
    "  # A range assigned to a partitioned JOIN.\n",
    "  min_=0, max_=10000, last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JOIN result table contains record addresses of `image_bytes` and `annotations_bytes`. The data value can be read as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space.core.ops.read import read_record_column\n",
    "\n",
    "images = read_record_column(image_mv.storage, joined_references, \"image_bytes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
