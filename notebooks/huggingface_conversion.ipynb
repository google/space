{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Between HuggingFace and Space Datasets\n",
    "\n",
    "The [HuggingFace hub](https://huggingface.co/docs/datasets-server/en/parquet) automatically converts every dataset to the Parquet format. The Parquet files can be appended to a Space dataset without rewriting data files. The metadata only append is performant and low-cost.\n",
    "\n",
    "Similarly, a Space dataset storing all fields in Parquet can be converted to a HuggingFace dataset, reusing existing data files. Therefore, users can use Space as a tool of data manipulation, materialized view, and version management for HuggingFace datasets.\n",
    "\n",
    "The `parquet_files` method lists all Parquet files of a HuggingFace dataset in the hub, with name `ds` (e.g., `ibm/duorc`), `splits`, and `config`. Replace the parameters for your dataset. See more details in the [HuggingFace docs](https://huggingface.co/docs/datasets-server/en/parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Optional\n",
    "\n",
    "# Change it to your HuggingFace API token.\n",
    "API_TOKEN = \"my_huggingface_api_token\"\n",
    "\n",
    "def parquet_files(ds: str, splits: Optional[List[str]] = None,\n",
    "    config: str = \"default\") -> List[str]:\n",
    "  global API_TOKEN\n",
    "\n",
    "  headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "  API_URL = f\"https://datasets-server.huggingface.co/parquet?dataset={ds}\"\n",
    "  response = requests.get(API_URL, headers=headers).json()\n",
    "  assert response[\"partial\"] == False\n",
    "\n",
    "  splits_set = set(splits) if splits else None\n",
    "  def filter_(f):\n",
    "    nonlocal splits_set, config\n",
    "    return (not splits_set or f[\"split\"] in splits_set) and f[\"config\"] == config\n",
    "\n",
    "  return [f[\"url\"] for f in response[\"parquet_files\"] if filter_(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Parquet files to a directory (local or Cloud Storage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/path/to/download/files\"\n",
    "\n",
    "for url in parquet_files(\"ibm/duorc\", splits=[\"train\"], config=\"ParaphraseRC\"):\n",
    "  os.system(f\"wget {url} -P {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the Parquet file schema, it will be used as the Space dataset schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "file_name = os.listdir(OUTPUT_DIR)[0]\n",
    "schema = pq.read_schema(os.path.join(OUTPUT_DIR, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty Space dataset. The schema is the same as the schema of downloaded Parquet files. Space requires a primary key, but it is not enforced. Uniqueness is required for insert and upsert operations. We simply choose `plot_id` as primary key for demo purpose. `append_parquet` appends the Parquet files into the Space dataset, by registrating these files into metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import Dataset, DirCatalog\n",
    "\n",
    "catalog = DirCatalog(\"/path/to/my/tables\")\n",
    "ds = catalog.create_dataset(\"huggingface_demo\",\n",
    "  schema, primary_keys=[\"plot_id\"], record_fields=[])\n",
    "\n",
    "# Append existing Parquet files into Space.\n",
    "# TODO: the files are outside the Space dataset's `data` folder;\n",
    "# to support an option to move/copy these files into the dataset\n",
    "# folder.\n",
    "ds.local().append_parquet(f\"{OUTPUT_DIR}/*.parquet\")\n",
    "\n",
    "print(ds.local().read_all(fields=[\"plot_id\"]).num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of manipulating data in a Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "# Delete rows.\n",
    "ds.local().delete(pc.field(\"plot_id\") == \"/m/03vyhn\")\n",
    "ds.add_tag(\"after_delete\")\n",
    "\n",
    "# Show all versions.\n",
    "print(ds.versions().to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `index_files` method to list Parquet files of a dataset version, and construct a HuggingFace dataset from the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_ds = load_dataset(\"parquet\",\n",
    "  data_files={\"train\": ds.index_files(version=\"after_delete\")})\n",
    "\n",
    "print(huggingface_ds.num_rows)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
