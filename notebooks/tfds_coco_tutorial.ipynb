{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Tensorflow COCO dataset\n",
    "\n",
    "[TFDS COCO dataset](https://www.tensorflow.org/datasets/catalog/coco) defines the following features structure `tf_features_dict`. It is used for serializing complex nested data into bytes, and deserialize it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow_datasets import features as f\n",
    "\n",
    "tf_features_dict = f.FeaturesDict({\n",
    " \"image\": f.Image(shape=(None, None, 3), dtype=np.uint8),\n",
    " \"objects\": f.Sequence({\n",
    "   \"area\": np.int64,\n",
    "   \"bbox\": f.BBoxFeature(),\n",
    "   \"id\": np.int64,\n",
    "   \"is_crowd\": np.bool_,\n",
    "   \"label\": f.ClassLabel(num_classes=80),\n",
    "  }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we move the COCO dataset from TFDS to Space. In addition, we copy the `objects` field above from the row-oriented files to Parquet files, so we can run SQL queries on it.\n",
    "\n",
    "The Space dataset's schema is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from space import TfFeatures  # A custom PyArrow type.\n",
    "\n",
    "object_schema = pa.struct([\n",
    "  (\"area\", pa.int64()),\n",
    "  (\"bbox\", pa.list_(pa.float32())),  # TODO: to use fixed size list.\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"is_crowd\", pa.bool_()),\n",
    "  (\"label\", pa.int64()),\n",
    "])\n",
    "\n",
    "ds_schema = pa.schema([\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"filename\", pa.string()),\n",
    "  (\"objects\", pa.list_(object_schema)),\n",
    "  (\"features\", TfFeatures(tf_features_dict))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_fields will be stored in ArrayRecord files.\n",
    "ds = Dataset.create(\"/path/to/space/mybucket/demo\",\n",
    "  ds_schema, primary_keys=[\"id\"], record_fields=[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a method `index_fn` that reads ArrayRecord files and builds indexes for it. The method returns three index fields (`id`, `filename`, `objects`) to be written into the Space dataset's Parquet files. At the same time, the row's address in the input ArrayRecord files are also persisted.\n",
    "\n",
    "Calling `load_array_record` will processes all ArrayRecord files in the folder `/path/to/tfds/coco/files` using this method. The COCO dataset is now under Space's management after the call completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "def index_fn(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  example = example[\"features\"][0]\n",
    "  return {\n",
    "    \"id\": example[\"image/id\"],\n",
    "    \"filename\": example[\"image/filename\"],\n",
    "    \"objects\": coco_utils.tf_objects_to_pylist(example[\"objects\"]),\n",
    "  }\n",
    "\n",
    "runner = ds.local()\n",
    "# \"/path/to/tfds/coco/files\" is where TFDS saves the downloaded\n",
    "# ArrayRecord files.\n",
    "runner.load_array_record(\"/path/to/tfds/coco/files\", index_fn)\n",
    "ds.add_tag(\"initialized\")  # Tag the current version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `objects` field in TFDS becomes a columnar field that can be analyzed via SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Load the \"objects\" column into memory as PyArrow and query using DuckDB.\n",
    "# The SQL query returns the largest object bbox area in the dataset.\n",
    "objects = runner.read_all(fields=[\"objects\"])\n",
    "duckdb.sql(\n",
    "  \"SELECT MAX(objs.area) FROM (SELECT unnest(objects) AS objs FROM objects)\"\n",
    ").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space supports data mutations; each modification generates a new version (`snapshot_id`). It supports reading any previous versions (time travel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "# Delete a row from a Space dataset. The mutation creates a new snapshot.\n",
    "runner.delete(pc.field(\"id\") == pc.scalar(361586))\n",
    "\n",
    "# Read the current version:\n",
    "runner.read()\n",
    "\n",
    "# Time travel back to before the deletion, by setting a read version.\n",
    "runner.read(version=\"initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from the Space dataset through a [random access data source interface](https://www.tensorflow.org/datasets/tfless_tfds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import RandomAccessDataSource\n",
    "\n",
    "datasource = RandomAccessDataSource(\n",
    "  # field-name: storage-location, for reading data from ArrayRecord files.\n",
    "  {\n",
    "    \"features\": \"/path/to/space/mybucket/demo\",\n",
    "  },\n",
    "  # Auto deserialize data using `tf_features_dict`.\n",
    "  deserialize=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
