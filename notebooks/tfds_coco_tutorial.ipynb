{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Tensorflow COCO dataset\n",
    "\n",
    "This example will load the [TFDS COCO dataset](https://www.tensorflow.org/datasets/catalog/coco) into Space without copying ArrayRecord files. We will demonstrate how to modify data and use SQL engine to analyze annotations.\n",
    "\n",
    "First let's download the COCO datasets in ArrayRecord format by following the [TFDS docs](https://www.tensorflow.org/datasets/tfless_tfds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.data_source('coco/2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFDS COCO dataset defines the following features structure `tf_features_dict`. It is used for serializing complex nested data into bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow_datasets import features as f\n",
    "\n",
    "tf_features_dict = f.FeaturesDict({\n",
    " \"image\": f.Image(shape=(None, None, 3), dtype=np.uint8),\n",
    " \"image/filename\": f.Text(),\n",
    " \"image/id\": np.int64,\n",
    " \"objects\": f.Sequence({\n",
    "   \"area\": np.int64,\n",
    "   \"bbox\": f.BBoxFeature(),\n",
    "   \"id\": np.int64,\n",
    "   \"is_crowd\": np.bool_,\n",
    "   \"label\": f.ClassLabel(num_classes=80),\n",
    "  }),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a copy of the above `objects` field into new Parquet files. This field will thus exist in both ArrayRecord files (original TFDS data, for feeding into training framework), and in Parquet for SQL queries. Note that the bulky image data is not copied to Parquet.\n",
    "\n",
    "The Space dataset's schema is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from space import TfFeatures  # A custom PyArrow type.\n",
    "\n",
    "# Equivalent to the `objects` field in the above FeaturesDict.\n",
    "object_schema = pa.struct([\n",
    "  (\"area\", pa.int64()),\n",
    "  (\"bbox\", pa.list_(pa.float32())),  # TODO: to use fixed size list.\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"is_crowd\", pa.bool_()),\n",
    "  (\"label\", pa.int64()),\n",
    "])\n",
    "\n",
    "ds_schema = pa.schema([\n",
    "  (\"id\", pa.int64()),\n",
    "  (\"filename\", pa.string()),\n",
    "  (\"objects\", pa.list_(object_schema)),  # A copy of `objects` in Parquet files.\n",
    "  (\"features\", TfFeatures(tf_features_dict))  # The original TFDS data.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new Space dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import Dataset\n",
    "\n",
    "ds_location = \"/directory/coco_demo\"  # Change it to your preferred location\n",
    "\n",
    "ds = Dataset.create(ds_location, ds_schema,\n",
    "  primary_keys=[\"id\"],\n",
    "  record_fields=[\"features\"])  # The `features` field is stored in ArrayRecord files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a method `index_fn` that reads ArrayRecord files and builds indexes for it. The method returns three index fields (`id`, `filename`, `objects`) to be written to Parquet files, together with the row's addresses in the ArrayRecord files. See the [storage design](../docs/design.md) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "def pydict_to_pylist(objects: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "  return [\n",
    "    {\"id\": area, \"area\": id_, \"bbox\": boxes, \"is_crowd\": is_crowds, \"label\": labels}\n",
    "    for area, id_, boxes, is_crowds, labels in\n",
    "    zip(objects[\"area\"], objects[\"id\"], objects[\"bbox\"], objects[\"is_crowd\"], objects[\"label\"])\n",
    "  ]\n",
    "\n",
    "def index_fn(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "  # Input format:\n",
    "  #   key: Space record field name, value: [deserialized TFDS value] (size is 1)\n",
    "  #    e.g., {\"features\": [{\"image\": v, \"image/id\": v, \"image/filename\": v, \"objects\": v}]}\n",
    "  example = example[\"features\"][0]\n",
    "  return {\n",
    "    \"id\": example[\"image/id\"],\n",
    "    \"filename\": example[\"image/filename\"],\n",
    "    \"objects\": pydict_to_pylist(example[\"objects\"]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `load_array_record` will processes all input ArrayRecord files using `index_fn` to obtain indexes. The loading will complete after the index fields have been written for all ArrayRecord records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDS downloaded files, replace it with your path\n",
    "input_pattern = \"/tensorflow_datasets/coco/2017/1.1.0/coco-validation.array_record*\"\n",
    "\n",
    "# ArrayRecord files.\n",
    "ds.local().append_array_record(input_pattern, index_fn)\n",
    "# >>>\n",
    "# JobResult(state=<State.SUCCEEDED: 1>, storage_statistics_update=num_rows: 5000\n",
    "# index_compressed_bytes: 31842\n",
    "# index_uncompressed_bytes: 47048\n",
    "# record_uncompressed_bytes: 816568313\n",
    "# , error_message=None)\n",
    "\n",
    "ds.add_tag(\"initialized\")  # Tag the current version.\n",
    "\n",
    "# Check loaded image IDs.\n",
    "image_ids = ds.local().read_all(fields=[\"id\"])\n",
    "image_ids.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects are stored in a columnar field `objects` now. Read it into memory as a PyArrow table, and use [DuckDB](https://github.com/duckdb/duckdb) SQL to query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Compute the min/max object bbox area in the dataset.\n",
    "sql = \"\"\"\n",
    "SELECT MIN(objs.area), MAX(objs.area) FROM (\n",
    "  SELECT UNNEST(objects) AS objs FROM objects)\n",
    "\"\"\"\n",
    "\n",
    "objects = ds.local().read_all(fields=[\"objects\"])\n",
    "duckdb.sql(sql).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space datasets are mutable. You can run append, insert, upsert, delete operations, locally (ds.local()) or distributedly (ds.ray()). A new snapshot of dataset is generated after a mutation, you can read previous snapshots by providing a snapshot ID or a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "# Delete a row from a Space dataset. The mutation creates a new snapshot\n",
    "ds.local().delete(pc.field(\"id\") == pc.scalar(361586))\n",
    "ds.add_tag(\"delete_some_data\")  # Tag the new snapshot\n",
    "\n",
    "# Check total rows\n",
    "ds.local().read_all(fields=[\"id\"]).num_rows\n",
    "# >>>\n",
    "# 4999\n",
    "\n",
    "# Time travel back to before the deletion, by setting a read version.\n",
    "ds.local().read_all(version=\"initialized\", fields=[\"id\"]).num_rows\n",
    "# >>>\n",
    "# 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data (the `features` field) in the original TFDS format via a [random access data source interface](https://www.tensorflow.org/datasets/tfless_tfds), as the input of training frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space import RandomAccessDataSource\n",
    "\n",
    "datasource = RandomAccessDataSource(\n",
    "  # field-name: storage-location, for reading data from ArrayRecord files.\n",
    "  {\"features\": ds_location},\n",
    "  deserialize=True,  # Auto deserialize data using `tf_features_dict`.\n",
    "  use_array_record_data_source=False)\n",
    "\n",
    "len(datasource)\n",
    "# >>>\n",
    "# 4999\n",
    "\n",
    "# Read the original TFDS data.\n",
    "datasource[11]\n",
    "# >>>\n",
    "# {'image': array([[[239, 239, 237],\n",
    "#        [239, 239, 241],\n",
    "#        [239, 239, 239],\n",
    "#        ...\n",
    "#        [245, 246, 240]]], dtype=uint8),\n",
    "#  'image/filename': b'000000292082.jpg', ...\n",
    "#  'bbox': array([[0.51745313, 0.30523437, 0.6425156 , 0.3789375 ], ...\n",
    "#  'id': array([ 294157,  467036, 1219153, 1840967, 1937564]),\n",
    "#  'is_crowd': array([False, False, False, False, False]),\n",
    "#   'label': array([27,  0,  0, 27, 56])}}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
